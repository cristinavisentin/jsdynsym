$environment = ds.e.navigation(
  arena = ds.arena.prepared(
  name = u_barrier;
    initialRobotXRange = m.range(min = 0.45; max = 0.55);
    initialRobotYRange = m.range(min = 0.7; max = 0.8)
  );
  relativeV = true;
  robotMaxV = 0.1;
  robotRadius = 0.01
)

$cumRewardPlot = ea.l.onExpDone(
  of = ea.plot.multi.xy(
    xSubplot = rl.f.runString(name = none; s = "_");
    ySubplot = rl.f.runString(name = none; s = "_");
    line = rl.f.runString(name = agent; s = "{run.agent.name}");
    x = rl.f.nOfEpisodes();
    y = ds.f.cumulatedReward(of = rl.f.lastOutcome())
  );
  consumers = [rl.c.saver(
    path = "../../Documenti/experiments/{name}/{startTime}/cum-reward.svg";
    of = ea.f.imagePlotter(type = svg)
  )];
  eCondition = predicate.divisibleBy(f = rl.f.nOfEpisodes(); divisor = 100)
)

rl.experiment(
  runs = (agent = (randomGenerator = (seed = [1:1:5]) * [m.defaultRG()])
  * (actorLearningRate = [0.0001; 0.0005; 0.00025])
  * (criticLearningRate = [0.005; 0.001; 0.005])
  * [
    ds.rl.linearActorCritic()
  ]) * [
    rl.run(
      tasks = [
        ds.srlat.fromNumericalEnvironment(
          environment = $environment;
          reward = ds.e.nav.reward.reaching()
        )
      ];
      dT = 0.1;
      tRange = m.range(min = 0; max = 30);
      stopCriterion = predicate.gt(t = 10000; f = rl.f.nOfEpisodes())
    )
  ];
  listeners = [
    rl.l.console(
      functions = [ea.f.timestamp()];
      episodeCondition = predicate.divisibleBy(f = rl.f.nOfEpisodes(); divisor = 100);
      onlyLast = true
    );
    $cumRewardPlot
  ]
)